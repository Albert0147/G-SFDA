{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import os.path as osp\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import network, loss\n",
    "from torch.utils.data import DataLoader\n",
    "from data_list import ImageList, ImageList_idx\n",
    "import random, pdb, math, copy\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Ours')\n",
    "parser.add_argument('--gpu_id',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='8',\n",
    "                    help=\"device id to run\")\n",
    "parser.add_argument('--s', type=int, default=0, help=\"source\")\n",
    "parser.add_argument('--t', type=int, default=1, help=\"target\")\n",
    "parser.add_argument('--max_epoch',\n",
    "                    type=int,\n",
    "                    default=15,\n",
    "                    help=\"max iterations\")\n",
    "parser.add_argument('--interval', type=int, default=15)\n",
    "parser.add_argument('--batch_size',\n",
    "                    type=int,\n",
    "                    default=64,\n",
    "                    help=\"batch_size\")\n",
    "parser.add_argument('--worker',\n",
    "                    type=int,\n",
    "                    default=4,\n",
    "                    help=\"number of workers\")\n",
    "parser.add_argument(\n",
    "    '--dset',\n",
    "    type=str,\n",
    "    default='visda-2017')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help=\"learning rate\")\n",
    "parser.add_argument('--net',\n",
    "                    type=str,\n",
    "                    default='resnet101')\n",
    "parser.add_argument('--seed', type=int, default=2020, help=\"random seed\")\n",
    "\n",
    "parser.add_argument('--bottleneck', type=int, default=256)\n",
    "parser.add_argument('--epsilon', type=float, default=1e-5)\n",
    "parser.add_argument('--layer',\n",
    "                    type=str,\n",
    "                    default=\"wn\",\n",
    "                    choices=[\"linear\", \"wn\"])\n",
    "parser.add_argument('--classifier',\n",
    "                    type=str,\n",
    "                    default=\"bn\",\n",
    "                    choices=[\"ori\", \"bn\"])\n",
    "parser.add_argument('--output', type=str, default='hat/target/')\n",
    "parser.add_argument('--output_src', type=str, default='hat/source/')\n",
    "parser.add_argument('--da',\n",
    "                    type=str,\n",
    "                    default='uda')\n",
    "parser.add_argument('--issave', type=bool, default=True)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dset == 'visda-2017':\n",
    "    names = ['train', 'validation']\n",
    "    args.class_num = 12\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n",
    "'''SEED = args.seed\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "'''\n",
    "for i in range(len(names)):\n",
    "    if i == args.s:\n",
    "        continue\n",
    "    args.t = i\n",
    "\n",
    "    folder = './data/'\n",
    "    args.s_dset_path = folder + args.dset + '/' + names[\n",
    "        args.s] + '_list.txt'\n",
    "    args.t_dset_path = folder + args.dset + '/' + names[\n",
    "        args.t] + '_list.txt'\n",
    "    args.test_dset_path = folder + args.dset + '/' + names[\n",
    "        args.t] + '_list.txt'\n",
    "\n",
    "    args.output_dir_src = osp.join(args.output_src, args.da, args.dset,\n",
    "                                    names[args.s][0].upper())\n",
    "    args.output_dir = osp.join(\n",
    "        args.output, args.da, args.dset,\n",
    "        names[args.s][0].upper() + names[args.t][0].upper())\n",
    "    args.name = names[args.s][0].upper() + names[args.t][0].upper()\n",
    "\n",
    "    if not osp.exists(args.output_dir):\n",
    "        os.system('mkdir -p ' + args.output_dir)\n",
    "    if not osp.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_train(resize_size=256, crop_size=224, alexnet=False):\n",
    "    if not alexnet:\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "    else:\n",
    "        normalize = Normalize(meanfile='./ilsvrc_2012_mean.npy')\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((resize_size, resize_size)),\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), normalize\n",
    "    ])\n",
    "\n",
    "\n",
    "def image_test(resize_size=256, crop_size=224, alexnet=False):\n",
    "    if not alexnet:\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "    else:\n",
    "        normalize = Normalize(meanfile='./ilsvrc_2012_mean.npy')\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((resize_size, resize_size)),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(), normalize\n",
    "    ])\n",
    "\n",
    "\n",
    "def data_load(args):\n",
    "    ## prepare data\n",
    "    dsets = {}\n",
    "    dset_loaders = {}\n",
    "    train_bs = args.batch_size\n",
    "    txt_src = open(args.s_dset_path).readlines()\n",
    "    txt_tar = open(args.t_dset_path).readlines()\n",
    "    txt_test = open(args.test_dset_path).readlines()\n",
    "\n",
    "    if not args.da == 'uda':\n",
    "        label_map_s = {}\n",
    "        for i in range(len(args.src_classes)):\n",
    "            label_map_s[args.src_classes[i]] = i\n",
    "\n",
    "        new_tar = []\n",
    "        for i in range(len(txt_tar)):\n",
    "            rec = txt_tar[i]\n",
    "            reci = rec.strip().split(' ')\n",
    "            if int(reci[1]) in args.tar_classes:\n",
    "                if int(reci[1]) in args.src_classes:\n",
    "                    line = reci[0] + ' ' + str(label_map_s[int(\n",
    "                        reci[1])]) + '\\n'\n",
    "                    new_tar.append(line)\n",
    "                else:\n",
    "                    line = reci[0] + ' ' + str(len(label_map_s)) + '\\n'\n",
    "                    new_tar.append(line)\n",
    "        txt_tar = new_tar.copy()\n",
    "        txt_test = txt_tar.copy()\n",
    "\n",
    "    dsize = len(txt_src)\n",
    "    tr_size = int(0.9*dsize)\n",
    "    # print(dsize, tr_size, dsize - tr_size)\n",
    "    tr_txt, te_txt = torch.utils.data.random_split(txt_src, [tr_size, dsize - tr_size])\n",
    "\n",
    "    dsets[\"source_tr\"] = ImageList(tr_txt, transform=image_train())\n",
    "    dset_loaders[\"source_tr\"] = DataLoader(dsets[\"source_tr\"],\n",
    "                                           batch_size=64,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=args.worker,\n",
    "                                           drop_last=False)\n",
    "    dsets[\"source_te\"] = ImageList(te_txt, transform=image_test())\n",
    "    dset_loaders[\"source_te\"] = DataLoader(dsets[\"source_te\"],\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=args.worker,\n",
    "                                           drop_last=False)\n",
    "    dsets[\"target\"] = ImageList_idx(txt_tar, transform=image_train())\n",
    "    dset_loaders[\"target\"] = DataLoader(dsets[\"target\"],\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=args.worker,\n",
    "                                        drop_last=False)\n",
    "    dsets[\"test\"] = ImageList_idx(txt_test, transform=image_test())\n",
    "    dset_loaders[\"test\"] = DataLoader(dsets[\"test\"],\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=args.worker,\n",
    "                                      drop_last=False)\n",
    "\n",
    "    return dset_loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hat/source/uda/visda-2017/T'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "args.output_dir_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "feat_classifier(\n",
       "  (fc): Linear(in_features=256, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "dset_loaders = data_load(args)\n",
    "## set base network\n",
    "netF = network.ResBase(res_name=args.net).cuda()\n",
    "netB = network.feat_bootleneck_sdaE(type=args.classifier,\n",
    "                                feature_dim=netF.in_features,\n",
    "                                bottleneck_dim=args.bottleneck).cuda()\n",
    "netC = network.feat_classifier(type=args.layer,\n",
    "                                class_num=args.class_num,\n",
    "                                bottleneck_dim=args.bottleneck).cuda()\n",
    "\n",
    "\n",
    "modelpath = 'hat/target/uda/visda-2017/TV/' + '/target_F_final.pt'\n",
    "netF.load_state_dict(torch.load(modelpath))\n",
    "modelpath = 'hat/target/uda/visda-2017/TV/' + '/target_B_final.pt'\n",
    "netB.load_state_dict(torch.load(modelpath))\n",
    "modelpath = 'hat/target/uda/visda-2017/TV/' + '/target_C_final.pt'\n",
    "netC.load_state_dict(torch.load(modelpath))\n",
    "netF.eval()\n",
    "netB.eval()\n",
    "netC.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Domain Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLS_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer=nn.Sequential(nn.Linear(256,64),nn.ReLU(),nn.Linear(64,2))\n",
    "    def forward(self,x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cls=CLS_D().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling data for training domain classifier\n",
    "loader_t = dset_loaders['target']\n",
    "iter_test_t = iter(loader_t)\n",
    "data_t = iter_test_t.next()\n",
    "#data_t = iter_test_t.next()\n",
    "data_t = iter_test_t.next()\n",
    "input_t = data_t[0].cuda()\n",
    "label_t = data_t[1].cuda()\n",
    "\n",
    "loader_s = dset_loaders['source_tr']\n",
    "iter_test_s = iter(loader_s)\n",
    "#data_s = iter_test_s.next()\n",
    "data_s = iter_test_s.next()\n",
    "data_s = iter_test_s.next()\n",
    "data_s = iter_test_s.next()\n",
    "input_s = data_s[0].cuda()\n",
    "label_s = data_s[1].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input_t=netB.bn(netB.bottleneck(netF(input_t)))\n",
    "    label_t=torch.ones(input_t.shape[0]).long().cuda()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input_s=netB.bn(netB.bottleneck(netF(input_s)))\n",
    "    label_s=torch.zeros(input_s.shape[0]).long().cuda()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.cat((input_t,input_s))\n",
    "labels=torch.cat((label_t,label_s))\n",
    "inputs_np=inputs.cpu().numpy()\n",
    "labels_np=labels.cpu().numpy()\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(inputs_np)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.from_numpy(inputs_np).cuda()\n",
    "labels=torch.from_numpy(labels_np).cuda().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward>) 0.5546875\n",
      "tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward>) 0.5703125\n",
      "tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward>) 0.6015625\n",
      "tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward>) 0.625\n",
      "tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward>) 0.671875\n",
      "tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward>) 0.7578125\n",
      "tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward>) 0.765625\n",
      "tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>) 0.84375\n",
      "tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward>) 0.8828125\n",
      "tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward>) 0.8984375\n",
      "tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward>) 0.890625\n",
      "tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward>) 0.8828125\n",
      "tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward>) 0.8828125\n",
      "tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward>) 0.8828125\n",
      "tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward>) 0.8984375\n",
      "tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>) 0.8984375\n",
      "tensor(0.5330, device='cuda:0', grad_fn=<NllLossBackward>) 0.8984375\n",
      "tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward>) 0.9140625\n",
      "tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>) 0.9140625\n",
      "tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>) 0.9140625\n",
      "tensor(0.5037, device='cuda:0', grad_fn=<NllLossBackward>) 0.9140625\n",
      "tensor(0.4967, device='cuda:0', grad_fn=<NllLossBackward>) 0.921875\n",
      "tensor(0.4898, device='cuda:0', grad_fn=<NllLossBackward>) 0.9296875\n",
      "tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4640, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4580, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4301, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4249, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>) 0.9296875\n",
      "tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>) 0.9296875\n",
      "tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3260, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>) 0.9375\n",
      "tensor(0.3196, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.3134, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.3073, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2930, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>) 0.953125\n",
      "tensor(0.2771, device='cuda:0', grad_fn=<NllLossBackward>) 0.9609375\n",
      "tensor(0.2746, device='cuda:0', grad_fn=<NllLossBackward>) 0.9609375\n",
      "tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>) 0.9609375\n",
      "tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>) 0.9609375\n",
      "tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>) 0.9609375\n",
      "tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>) 0.9609375\n",
      "tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>) 0.96875\n",
      "tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>) 0.9765625\n",
      "tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>) 0.984375\n",
      "tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>) 0.984375\n",
      "tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>) 0.984375\n",
      "tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>) 0.984375\n",
      "tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>) 0.984375\n",
      "tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>) 0.984375\n",
      "tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>) 0.984375\n",
      "tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>) 0.9921875\n",
      "tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n",
      "tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>) 1.0\n"
     ]
    }
   ],
   "source": [
    "optim=torch.optim.SGD(d_cls.parameters(),lr=0.01)\n",
    "for i in range(200):\n",
    "    output=d_cls(inputs)\n",
    "    loss=nn.CrossEntropyLoss()(output,labels)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    output=d_cls(inputs)\n",
    "    _,pred=torch.max(output,1)\n",
    "    accuracy = torch.sum(\n",
    "        torch.squeeze(pred).float() == labels).item() / float(\n",
    "            labels.size()[0])\n",
    "    print(loss,accuracy)"
   ]
  },
  {
   "source": [
    "## accuracy of domain classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = True\n",
    "loader=dset_loaders['source_te']\n",
    "with torch.no_grad():\n",
    "    iter_test = iter(loader)\n",
    "    for i in range(len(loader)):\n",
    "        data = iter_test.next()\n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "        inputs = inputs.cuda()\n",
    "        outputs = netB.bn(netB.bottleneck(netF(inputs)))\n",
    "        outputs=d_cls(outputs)\n",
    "        if start_test:\n",
    "            all_output = outputs.float().cpu()\n",
    "            all_label = labels.float()\n",
    "            start_test = False\n",
    "        else:\n",
    "            all_output = torch.cat((all_output, outputs.float().cpu()), 0)\n",
    "            all_label = torch.cat((all_label, labels.float()), 0)\n",
    "all_label=torch.zeros_like(all_label).long()\n",
    "_, predict = torch.max(all_output, 1)\n",
    "accuracy = torch.sum(\n",
    "    torch.squeeze(predict).float() == all_label).item() / float(\n",
    "        all_label.size()[0])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = True\n",
    "loader=dset_loaders['test']\n",
    "with torch.no_grad():\n",
    "    iter_test = iter(loader)\n",
    "    for i in range(len(loader)):\n",
    "        data = iter_test.next()\n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "        inputs = inputs.cuda()\n",
    "        outputs = netB.bn(netB.bottleneck(netF(inputs)))\n",
    "        outputs=d_cls(outputs)\n",
    "        if start_test:\n",
    "            all_output = outputs.float().cpu()\n",
    "            all_label = labels.float()\n",
    "            start_test = False\n",
    "        else:\n",
    "            all_output = torch.cat((all_output, outputs.float().cpu()), 0)\n",
    "            all_label = torch.cat((all_label, labels.float()), 0)\n",
    "all_label=torch.ones_like(all_label).long()\n",
    "_, predict = torch.max(all_output, 1)\n",
    "accuracy = torch.sum(\n",
    "    torch.squeeze(predict).float() == all_label).item() / float(\n",
    "        all_label.size()[0])\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "## Accuracy on source and target domain with the estimated domain ID"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = True\n",
    "loader=dset_loaders['source_te']\n",
    "with torch.no_grad():\n",
    "    iter_test = iter(loader)\n",
    "    for i in range(len(loader)):\n",
    "        data = iter_test.next()\n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "        inputs = inputs.cuda()\n",
    "        outputs = netB.bn(netB.bottleneck(netF(inputs)))\n",
    "        outputs=d_cls(outputs)\n",
    "        idx=torch.max(outputs,1)[-1].long().item()\n",
    "        output_f, masks = netB(netF(inputs),t=idx)\n",
    "        output = netC(output_f)\n",
    "        softmax_out = nn.Softmax(dim=1)(output)\n",
    "        if start_test:\n",
    "            all_output = output.float().cpu()\n",
    "            all_label = labels.float()\n",
    "            start_test = False\n",
    "        else:\n",
    "            all_output = torch.cat((all_output, output.float().cpu()), 0)\n",
    "            all_label = torch.cat((all_label, labels.float()), 0)\n",
    "_, predict = torch.max(all_output, 1)\n",
    "matrix = confusion_matrix(all_label, torch.squeeze(predict).float())\n",
    "acc = matrix.diagonal() / matrix.sum(axis=1) * 100\n",
    "aacc = acc.mean()\n",
    "aa = [str(np.round(i, 2)) for i in acc]\n",
    "acc = ' '.join(aa)\n",
    "print(aacc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = True\n",
    "loader=dset_loaders['test']\n",
    "with torch.no_grad():\n",
    "    iter_test = iter(loader)\n",
    "    for i in range(len(loader)):\n",
    "        data = iter_test.next()\n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "        inputs = inputs.cuda()\n",
    "        outputs = netB.bn(netB.bottleneck(netF(inputs)))\n",
    "        outputs=d_cls(outputs)\n",
    "        idx=torch.max(outputs,1)[-1].long().item()\n",
    "        output_f, masks = netB(netF(inputs),t=idx)\n",
    "        output = netC(output_f)\n",
    "        softmax_out = nn.Softmax(dim=1)(output)\n",
    "        if start_test:\n",
    "            all_output = output.float().cpu()\n",
    "            all_label = labels.float()\n",
    "            start_test = False\n",
    "        else:\n",
    "            all_output = torch.cat((all_output, output.float().cpu()), 0)\n",
    "            all_label = torch.cat((all_label, labels.float()), 0)\n",
    "_, predict = torch.max(all_output, 1)\n",
    "matrix = confusion_matrix(all_label, torch.squeeze(predict).float())\n",
    "acc = matrix.diagonal() / matrix.sum(axis=1) * 100\n",
    "aacc = acc.mean()\n",
    "aa = [str(np.round(i, 2)) for i in acc]\n",
    "acc = ' '.join(aa)\n",
    "print(aacc, acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}